{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Docker Spark setup\n",
    "\n",
    "This notebook is meant to run on a spark 2 docker container. First i'll describe the steps to set it up.\n",
    "\n",
    "On a Linux based system install Docker and Docker-compose.Create this file : docker-compose.yml. The contents is listed below.  Then run: docker-compose build . Afterwards run this command : docker-compose build -d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "version: \"2\"\n",
    "\n",
    "services:\n",
    "  master:\n",
    "    image: singularities/spark\n",
    "    command: start-spark master\n",
    "    hostname: master\n",
    "    ports:\n",
    "      - \"6066:6066\"\n",
    "      - \"7070:7070\"\n",
    "      - \"8080:8080\"\n",
    "      - \"50070:50070\"\n",
    "      - \"8888:8888\"\n",
    "  worker:\n",
    "    image: singularities/spark\n",
    "    command: start-spark worker master\n",
    "    environment:\n",
    "      SPARK_WORKER_CORES: 1\n",
    "      SPARK_WORKER_MEMORY: 2g\n",
    "    links:\n",
    "      - master\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "With docker ps , check if the master and worker containers are running.\n",
    "Connect to the master node:\n",
    "docker exec -it [container id master] bash\n",
    "On the master node continue with setting up as described below.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark and conda env setup\n",
    "\n",
    "```\n",
    "First install Anaconda 4 (latest version) on the Docker container with Spark Master. Then install a new Conda environment for Spark, using python 3.5 (3.6 has a bug).  \n",
    "\n",
    "conda create -n spark python=3.5\n",
    "source activate spark\n",
    "conda install notebook ipykernel\n",
    "ipython kernel install --user --name spark --display-name spark\n",
    "\n",
    "Make jupyter start script, and run it:\n",
    "PYSPARK_PYTHON=/root/anaconda3/envs/spark/bin/python\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=0.0.0.0 --port=8888' $SPARK_HOME/bin/pyspark\n",
    "\n",
    "Now go to the url it gives (http://0.0.0.0:8888/<some code>)\n",
    ", Run the nodebook sections.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Start this in spark conda env to test\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example data\n",
    "\n",
    "```\n",
    "This example works if you clone https://github.com/PacktPublishing/Learning-PySpark\n",
    "\n",
    "and make sure its in /root/learningPySpark on the Docker container with Spark Master. \n",
    "\n",
    "To install git on this container run command: apt-get install git\n",
    ", on github (or bitbucket) create a repository so you can save changes from the container and push it to Github. Use the following commands on the Docker container to init and push the data :\n",
    "\n",
    "git init\n",
    "git add <your file>\n",
    "git commit -m \"first commit\"\n",
    "git remote add origin https://github.com/michelnossin/pyspark_training_docker.git\n",
    "git push -u origin master\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RANDOM FLIGHTS SET, AND WORLD AIRPORT SET\n",
    "flights = \"file:/root/learningPySpark/Chapter03/flight-data/departuredelays.csv\" \n",
    "airports = \"file:/root/learningPySpark/Chapter03/flight-data/airport-codes-na.txt\" \n",
    "airports_df = spark.read.csv(airports,header='true',inferSchema='true',sep='\\t')\n",
    "airports_df.createOrReplaceTempView(\"airports\")\n",
    "flights_df = spark.read.csv(flights,header='true')\n",
    "flights_df.createOrReplaceTempView(\"flights\")\n",
    "flights_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[na: string, altitude: string, dest: string, heading: string, flight: string, na2: string, landed: string, time: string, lat: string, lon: string, na3: string, org: string, na4: string, registration: string, flight2: string, speed: string, na6: string, planetype: string, altitude_delta: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RADAR TRACK\n",
    "track_file = \"file:/root/fr24/fr24_20160624.csv\"\n",
    "header=['na','altitude','dest','heading','flight','na2','landed','time','lat',\\\n",
    "         'lon','na3','org','na4','registration','flight2','speed','na6','planetype', 'altitude_delta']\n",
    "fields = [ *[\n",
    "           typ.StructField(h, typ.StringType(), True)\n",
    "           for h in header\n",
    "       ]\n",
    "   ]\n",
    "schema = typ.StructType(fields)\n",
    "schema   \n",
    "tracks_df = spark.read.csv(track_file,header='false',schema=schema)\n",
    "\n",
    "#filter tracks early to make it speed up\n",
    "tracks_df = tracks_df.where(\"dest == 'AMS'\") #14 milj -> 114k\n",
    "tracks_df.createOrReplaceTempView(\"tracks\")\n",
    "tracks_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Alternatively use correct schema from beginning:\n",
    "import pyspark.sql.types as typ\n",
    "   labels = [\n",
    "       ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n",
    "       ('BIRTH_PLACE', typ.StringType()),\n",
    "       ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "       ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "       ('CIG_BEFORE', typ.IntegerType()),\n",
    "       ('CIG_1_TRI', typ.IntegerType()),\n",
    "       ('CIG_2_TRI', typ.IntegerType()),\n",
    "       ('CIG_3_TRI', typ.IntegerType()),\n",
    "       ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "       ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "       ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "       ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "       ('DIABETES_PRE', typ.IntegerType()),\n",
    "       ('DIABETES_GEST', typ.IntegerType()),\n",
    "       ('HYP_TENS_PRE', typ.IntegerType()),\n",
    "       ('HYP_TENS_GEST', typ.IntegerType()),\n",
    "       ('PREV_BIRTH_PRETERM', typ.IntegerType())\n",
    "   ]\n",
    "   schema = typ.StructType([\n",
    "       typ.StructField(e[0], e[1], False) for e in labels\n",
    "   ])\n",
    "   births = spark.read.csv('births_transformed.csv.gz',\n",
    "                           header=True,\n",
    "                           schema=schema)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## First look at data:\n",
    "\n",
    "```\n",
    "source activate spark\n",
    "python -m pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(1) from flights\").show()\n",
    "spark.sql(\"select count(1) from airports\").show()\n",
    "spark.sql(\"select count(1) from tracks\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flights_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "airports_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tracks_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "airports_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flights_df.printSchema() #date, delay and distance should change to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tracks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cleaning data\n",
    "\n",
    "Your data can be stained with duplicates, missing observations and outliers, non- existent addresses, wrong phone numbers and area codes, inaccurate geographical coordinates, wrong dates, incorrect labels, mixtures of upper and lower cases, trailing spaces, and many other more subtle problems. It is your job to clean it, irrespective of whether you are a data scientist or data engineer,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Duplicate rows check and remove\n",
    "First lets define some our spark util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def showDuplicateRowsCount(df):\n",
    "    'Show row count with full duplicated rows'\n",
    "    print(\"====Checking table duplicate rows =====\")\n",
    "    print('Count of rows: {0}'.format(df.count()))\n",
    "    print('Count of distinct rows: {0}'.format(df.distinct().count()))\n",
    "    print('===> nr of duplicate rows {0}'.format(df.count()-df.distinct().count()))\n",
    "def showDuplicatesColumnCount(df,col):\n",
    "    'Show duplicate rows based on a specific (id) col.'\n",
    "    print(\"=====Checking col {0}\".format(col))\n",
    "    print('Count of values: {0}'.format(df.count()))\n",
    "    distinct_col_count = df.select([\n",
    "           c for c in df.columns if c != col\n",
    "       ]).distinct().count()\n",
    "    print('Count of distinct column values: {0}'.format(distinct_col_count))\n",
    "    print (\"====> duplicate count {0}\".format(df.count() - distinct_col_count))\n",
    "def showDuplicatesColumnCountSpark(df,col):\n",
    "    'spark version of Showduplicatescolumncount()'\n",
    "    df.agg(\n",
    "       fn.count(col).alias('count'),\n",
    "       fn.countDistinct(col).alias('distinct')\n",
    "    ).show()\n",
    "def showDuplicateColumnsCount(df):\n",
    "    'Show duplicate rows based on all columns in a dataframe'\n",
    "    for col in df.columns:\n",
    "        showDuplicatesColumn(df,col)\n",
    "def dropDuplicateColumn(df,col):\n",
    "    'drop rows with duplicate columns based on certain (id) column'\n",
    "    df = df.dropDuplicates(subset=[\n",
    "       c for c in df.columns if c != col\n",
    "    ])\n",
    "#   \n",
    "#def getDFDuplicateColumns(df,col,new_col):\n",
    "#    uniq_df = df.select([\n",
    "#           c for c in df.columns if c != col\n",
    "#       ]).distinct()\n",
    "#    duplicate_df = df.subtract(uniq_df)\n",
    "#    \n",
    "#    return(duplicate_df.withColumn(new_col, \\\n",
    "#                            fn.monotonically_increasing_id()))\n",
    "#   **/ \n",
    "def showMissingDataPercent(df_miss):\n",
    "    'show each column and percentage of missing data, 0 - 1 , 0 means no missing data'\n",
    "    df_miss.agg(*[\n",
    "       (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n",
    "       for c in df_miss.columns\n",
    "    ]).show()\n",
    "def getDFDropColumn(df_miss,col):\n",
    "    'Get a new dataframe based on another without given column'\n",
    "    return(df_miss.select([\n",
    "       c for c in df_miss.columns if c != col\n",
    "    ]))\n",
    "def getDFDropMissingRows(df_miss):\n",
    "    'Drop rows with any missing column field'\n",
    "    return(df_miss.dropna())\n",
    "def fillMissingMeanColumn(df,col):\n",
    "    'Fill in missing values in a certain column containing numerical data'\n",
    "    means = df.agg(\n",
    "       *[fn.mean(col).alias(col)\n",
    "           for c in df.columns if c != col]\n",
    "   ).toPandas().to_dict('records')[0]\n",
    "def getDFFillMissingCategoryColumn(df,col):\n",
    "    'Fill in missing values in a column containing a category and return df'\n",
    "    miss_dict = {col: \"missing\"}\n",
    "    return(df.fillna(miss_dict))\n",
    "def getDictOutliers(df_outliers,col_list):\n",
    "    'return dictionary with outliers boundaries , based on columns in list'\n",
    "    bounds = {}\n",
    "    for col in col_list:\n",
    "        quantiles = df_outliers.approxQuantile(\n",
    "           col, [0.25, 0.75], 0.05\n",
    "       )\n",
    "        IQR = quantiles[1] - quantiles[0]\n",
    "        bounds[col] = [\n",
    "           quantiles[0] - 1.5 * IQR,\n",
    "           quantiles[1] + 1.5 * IQR\n",
    "     ]\n",
    "    return bounds\n",
    "def getDFOutliers(df_outliers,bounds,cols,id_col):\n",
    "    'print all outlier rows based on dictionary with outlier bounderies dict, for columns in column list'\n",
    "    outliers = df_outliers.select(*[id_col] + [\n",
    "       (\n",
    "           (df_outliers[c] < bounds[c][0]) |\n",
    "           (df_outliers[c] > bounds[c][1])\n",
    "       ).alias(c + '_o') for c in cols\n",
    "    ])\n",
    "    return outliers\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Check duplicates rows, same value?\n",
    "flights_df = spark.sql(\"select * from flights\") #507 out of 1.4 milj\n",
    "showDuplicateRowsCount(flights_df)\n",
    "airports_df = spark.sql(\"select * from airports\") #0\n",
    "showDuplicateRowsCount(airports_df)\n",
    "tracks_df = spark.sql(\"select * from tracks\") #192k out of 14m, 735 out of 114k after filtering for AMS arrival\n",
    "showDuplicateRowsCount(tracks_df) #takes 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Change type of integer based columns , so we check outliers later on\n",
    "flights_df = flights_df.withColumn(\"delay\",flights_df[\"delay\"].cast(typ.IntegerType()))\n",
    "flights_df = flights_df.withColumn(\"distance\",flights_df[\"distance\"].cast(typ.IntegerType()))\n",
    "                   \n",
    "tracks_df = tracks_df.withColumn(\"altitude\",tracks_df[\"altitude\"].cast(typ.IntegerType()))  \n",
    "tracks_df = tracks_df.withColumn(\"altitude_delta\",tracks_df[\"altitude_delta\"].cast(typ.IntegerType()))\n",
    "tracks_df = tracks_df.withColumn(\"speed\",tracks_df[\"speed\"].cast(typ.IntegerType()))      \n",
    "tracks_df = tracks_df.withColumn(\"heading\",tracks_df[\"heading\"].cast(typ.IntegerType()))   \n",
    "tracks_df = tracks_df.withColumn(\"lat\",tracks_df[\"lat\"].cast(typ.FloatType()))  \n",
    "tracks_df = tracks_df.withColumn(\"lon\",tracks_df[\"lon\"].cast(typ.FloatType())) \n",
    "tracks_df = tracks_df.withColumn(\"time\",tracks_df[\"time\"].cast(typ.LongType())) \n",
    "tracks_df = tracks_df.withColumn(\"landed\",tracks_df[\"landed\"].cast(typ.IntegerType()))\n",
    "\n",
    "#Lets add a id columns for the flights\n",
    "flights_df = flights_df.withColumn('id',fn.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pure duplicates just drop these, but the flights tables might be different flights. We donts know without id\n",
    "tracks_df =tracks_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Duplicate columns check\n",
    "\n",
    "Some times there are columns identifying a row, and which are different.\n",
    "However in case you know the rest of the columns is the same you might want to remove these rows. eg , Michel , 1.90, hoofddorp , and michel2, 1.90, hoofddorp . Its the same person but id is incorrect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#airports IATA should be uniq. It seems 15 rows have identical data \n",
    "#but different IATA code\n",
    "showDuplicatesColumnCount(airports_df,'IATA')\n",
    "showDuplicatesColumnCountSpark(airports_df,'IATA')\n",
    "#TODO WHY ARE RESULT DIFFERENT!!!!!! SHOULD BE BOTH 511 OR 524!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "showDuplicatesColumnCount(tracks_df,'flight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "We could call dropDuplicateColumn(df_airports,'IATA')\n",
    "\n",
    "However this would delete rows without knowing the correct IATA. \n",
    "The Flights tables does not have uniq field like flightname,\n",
    "so will not delete any rows there either.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO: Make function to show these rows so we know which are duplicates\n",
    "#df_duplicate_airports = getDFDuplicateColumns(airports_df,'IATA','new_id')\n",
    "#df_duplicate_airports.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Missing data\n",
    "\n",
    "```\n",
    "Drop data row if possible in case of missing. if datasize. < 50% check which features are missing, and just drop these.\n",
    "Alternative impute missing:\n",
    "Boolean: add missing category\n",
    "categorial already: add multiple extra levels and and missing there\n",
    "numeric and ordinal: mean, median etc to fill in\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#0 = perfect, 1 = all is missing\n",
    "showMissingDataPercent(airports_df) #State misses some data\n",
    "showMissingDataPercent(flights_df)\n",
    "showMissingDataPercent(tracks_df) #We miss some, flight a bit, but is important to have these,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We could just drop column state, we keep all our rows, and have no missing data\n",
    "df_no_state = getDFDropColumn(airports_df,'State')\n",
    "showMissingDataPercent(df_no_state)\n",
    "\n",
    "df_no_flight = getDFDropColumn(tracks_df,'flight')\n",
    "showMissingDataPercent(df_no_flight)\n",
    "\n",
    "df_no_flight.count() #113993 out of 114k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------+------------+\n",
      "|City_missing|State_missing|Country_missing|IATA_missing|\n",
      "+------------+-------------+---------------+------------+\n",
      "|         0.0|          0.0|            0.0|         0.0|\n",
      "+------------+-------------+---------------+------------+\n",
      "\n",
      "+----------+----------------+------------+---------------+--------------+-----------+--------------+------------+-----------+-----------+-----------+-----------+-----------+--------------------+---------------+-------------+-----------+-----------------+----------------------+\n",
      "|na_missing|altitude_missing|dest_missing|heading_missing|flight_missing|na2_missing|landed_missing|time_missing|lat_missing|lon_missing|na3_missing|org_missing|na4_missing|registration_missing|flight2_missing|speed_missing|na6_missing|planetype_missing|altitude_delta_missing|\n",
      "+----------+----------------+------------+---------------+--------------+-----------+--------------+------------+-----------+-----------+-----------+-----------+-----------+--------------------+---------------+-------------+-----------+-----------------+----------------------+\n",
      "|       0.0|             0.0|         0.0|            0.0|           0.0|        0.0|           0.0|         0.0|        0.0|        0.0|        0.0|        0.0|        0.0|                 0.0|            0.0|          0.0|        0.0|              0.0|                   0.0|\n",
      "+----------+----------------+------------+---------------+--------------+-----------+--------------+------------+-----------+-----------+-----------+-----------+-----------+--------------------+---------------+-------------+-----------+-----------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "113167"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Or drop only rows with any missing data\n",
    "df_without_missing = getDFDropMissingRows(airports_df)\n",
    "showMissingDataPercent(df_without_missing)\n",
    "\n",
    "df_without_missing_flight = getDFDropMissingRows(tracks_df)\n",
    "showMissingDataPercent(df_without_missing_flight)\n",
    "\n",
    "df_without_missing_flight.count() #Also 113167 , so we could just use this for the tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-------+----+\n",
      "|         City|  State|Country|IATA|\n",
      "+-------------+-------+-------+----+\n",
      "|Washington DC|missing|    USA| IAD|\n",
      "|Washington DC|missing|    USA| DCA|\n",
      "|Washington DC|missing|    USA| WAS|\n",
      "+-------------+-------+-------+----+\n",
      "\n",
      "+------------+-------------+---------------+------------+\n",
      "|City_missing|State_missing|Country_missing|IATA_missing|\n",
      "+------------+-------------+---------------+------------+\n",
      "|         0.0|          0.0|            0.0|         0.0|\n",
      "+------------+-------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Or we can impute values, as this is a category we will add a missing category\n",
    "df_missing_state = getDFFillMissingCategoryColumn(airports_df,'State')\n",
    "df_missing_state.where(\"State == 'missing'\").show() #3\n",
    "df_missing_state.count() #526\n",
    "showMissingDataPercent(df_missing_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#LETS PICK LAST OPTION for Airports and trackers\n",
    "airports_df = df_missing_state\n",
    "tracks_df = df_without_missing_flight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### outliers\n",
    "\n",
    "Outliers are those observations that deviate signi cantly from the distribution of the rest of your sample. The de nitions of signi cance vary, but in the most general form, you can accept that there are no outliers if all the values are roughly within the Q1âˆ’1.5IQR and Q3+1.5IQR range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Show the ouytlier ranges for our integer based columns\n",
    "col_list = ['delay','distance'] \n",
    "\n",
    "#Run cast code in the beginning again (dont no why thats needed?)\n",
    "outliers_dict = getDictOutliers(flights_df,col_list)\n",
    "print(outliers_dict) \n",
    "\n",
    "#Show the ouytlier ranges for our integer based columns\n",
    "col_flights_list = ['lat','lon','altitude','heading','time','landed'] \n",
    "\n",
    "outliers_flights_dict = getDictOutliers(tracks_df,col_flights_list)\n",
    "print(outliers_flights_dict) #Not really handy way to check outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Flag rows\n",
    "#Run the id add function again, for some reason..\n",
    "df_outliers = getDFOutliers(flights_df,outliers_dict,col_list,'id')\n",
    "df_outliers.show()\n",
    "\n",
    "df_flight_outliers = getDFOutliers(tracks_df,outliers_flights_dict,col_flights_list,'flight')\n",
    "df_flight_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Show outlier  flights\n",
    "#1.4 milj flights, about 162k has outlier delays. And 75k outlier distance\n",
    "df_out= flights_df.join(df_outliers, on='id')\n",
    "print(df_out.filter('delay_o').select('id', 'delay').count())\n",
    "print(df_out.filter('distance_o').select('id', 'distance').count())\n",
    "df_out.filter('delay_o').select('id', 'delay').show()\n",
    "df_out.filter('distance_o').select('id', 'distance').show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Show outlier  tracks, dont understand the result yet .. todo\n",
    "df_out=tracks_df.join(df_flight_outliers, on='flight')\n",
    "print(df_out.filter('heading_o').select('flight', 'heading').count())  #None, however heading has strange values\n",
    "print(df_out.filter('altitude_o').select('flight', 'altitude').count()) #None, but shows some strange numbers\n",
    "print(df_out.filter('lat_o').select('flight', 'lat').count()) #32, < 33 but still good value\n",
    "print(df_out.filter('lon_o').select('flight', 'lon').count()) #-66 also good\n",
    "print(df_out.filter('landed_o').select('flight', 'landed').count())  # 0 , <> 0.0 .. \n",
    "\n",
    "df_out.filter('heading_o').select('flight', 'heading').show()\n",
    "df_out.filter('altitude_o').select('flight', 'altitude').show()\n",
    "df_out.filter('lat_o').select('flight', 'lat').show()\n",
    "df_out.filter('lon_o').select('flight', 'lon').show()\n",
    "df_out.filter('landed_o').select('flight', 'landed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lets filters in between known ranges:\n",
    "#landed 0 or 1\n",
    "#heading 0 - 360\n",
    "#altitude < 100 , > 50000 \n",
    "#the valid range of latitude in degrees is -90 and +90 . Longitude is in the range -180 and +180 \n",
    "tracks_df = tracks_df.where(\"landed == 0 or landed == 1\")\n",
    "tracks_df = tracks_df.where(\"heading >= 0 and heading < 360\")\n",
    "tracks_df = tracks_df.where(\"altitude > -100 and altitude < 50000\")\n",
    "tracks_df = tracks_df.where(\"lat >= -90 and lat <= 90\")\n",
    "tracks_df = tracks_df.where(\"lon >= -180 and lon <= 180\")\n",
    "tracks_df.count() #only 1 row removed 113992\n",
    "\n",
    "tracks_df.describe().toPandas() #looks fine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_sel = tracks_df.where(\"flight == 'KL836'\").toPandas().sort_values(['time']).reset_index() #515 rows\n",
    "df_sel.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import folium\n",
    "\n",
    "%matplotlib inline\n",
    "def inline_map(map):\n",
    "    \"\"\"\n",
    "    Embeds the HTML source of the map directly into the IPython notebook.\n",
    "    \n",
    "    This method will not work if the map depends on any files (json data). Also this uses\n",
    "    the HTML5 srcdoc attribute, which may not be supported in all browsers.\n",
    "    \"\"\"\n",
    "    map._build_map()\n",
    "    return HTML('<iframe srcdoc=\"{srcdoc}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(srcdoc=map.HTML.replace('\"', '&quot;')))\n",
    " \n",
    "def embed_map(map, path=\"map.html\"):\n",
    "    \"\"\"\n",
    "    Embeds a linked iframe to the map into the IPython notebook.\n",
    "    \n",
    "    Note: this method will not capture the source of the map into the notebook.\n",
    "    This method should work for all maps (as long as they use relative urls).\n",
    "    \"\"\"\n",
    "    map.create_map(path=path)\n",
    "    return HTML('<iframe src=\"files/{path}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(path=path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.style.use('ggplot')\n",
    "df_sel[['altitude','speed']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import folium\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def display(m, height=300):\n",
    "    \"\"\"Takes a folium instance and embed HTML.\"\"\"\n",
    "    m._build_map()\n",
    "    srcdoc = m.HTML.replace('\"', '&quot;')\n",
    "    embed = HTML('<iframe srcdoc=\"{0}\" '\n",
    "                 'style=\"width: 100%; height: {1}px; '\n",
    "                 'border: none\"></iframe>'.format(srcdoc, height))\n",
    "    return embed\n",
    "\n",
    "def inline_map(map):\n",
    "    \"\"\"\n",
    "    Embeds the HTML source of the map directly into the IPython notebook.\n",
    "    \n",
    "    This method will not work if the map depends on any files (json data). Also this uses\n",
    "    the HTML5 srcdoc attribute, which may not be supported in all browsers.\n",
    "    \"\"\"\n",
    "    map._build_map()\n",
    "    return HTML('<iframe srcdoc=\"{srcdoc}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(srcdoc=map.HTML.replace('\"', '&quot;')))\n",
    "\n",
    "def embed_map(map, path=\"map.html\"):\n",
    "    \"\"\"\n",
    "    Embeds a linked iframe to the map into the IPython notebook.\n",
    "    \n",
    "    Note: this method will not capture the source of the map into the notebook.\n",
    "    This method should work for all maps (as long as they use relative urls).\n",
    "    \"\"\"\n",
    "    #map.create_map(path=path)\n",
    "    return HTML('<iframe src=\"files/{path}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(path=path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import HTML\n",
    "def plotFlight(flight):\n",
    "    df_sel = tracks_df.where(\"flight == '\" + flight + \"'\").toPandas().sort_values(['time']).reset_index()\n",
    "    #df_sel = join_df.toPandas().query(\"flight == '\" + flight + \"'\").sort_values(['time']).reset_index()\n",
    "    fmap=folium.Map(location=[52.308871, 4.761392], zoom_start=4)\n",
    "    #for row in df_sel.iterrows():\n",
    "     #   latlon = [ row[1]['lat'], row[1]['lon'] ]\n",
    "    #   folium.Marker(latlon, popup=str(row[1]['time'])).add_to(fmap)\n",
    "     #   fmap.add_children\n",
    "    \n",
    "    \n",
    "    latlist = df_sel['lat'].tolist()\n",
    "    lonlist = df_sel['lon'].tolist()\n",
    "    coordinates = zip(latlist[:], lonlist[:])\n",
    "    line=folium.PolyLine(locations=coordinates,weight=3,color = 'red')\n",
    "    fmap.add_children(line)\n",
    "    fmap.save('osm.html')\n",
    "    return HTML('<iframe src=\"files/{path}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(path='osm.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#plotFlight('Y87486')\n",
    "plotFlight('KL214')\n",
    "plotFlight('KL836')\n",
    "#plotFlight('U26771')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read in the file\n",
    "flights = \"file:/root/learningPySpark/Chapter03/flight-data/departuredelays.csv\" \n",
    "fl = sc.textFile(flights) #you can use .gz, so better then spark sql\n",
    "header = fl.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Filter numeric columns in flights\n",
    "fl_filter = fl.filter(lambda row: row != header) \\\n",
    "       .map(lambda row: [int(elem) for elem in row.split(',') if (elem.isdigit() or elem.lstrip(\"-\").isdigit()) ])\n",
    "fl_filter.take(5) #.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create schema\n",
    "fields = [ *[\n",
    "           typ.StructField(h, typ.IntegerType(), True)\n",
    "           for h in header.split(',')\n",
    "       ]\n",
    "   ]\n",
    "schema = typ.StructType(fields)\n",
    "schema   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create DF Spark\n",
    "fli_df = spark.createDataFrame(fl_filter, schema)\n",
    "fli_df.printSchema()\n",
    "#fli_df.show() Some columns are not integer so crash, to fix later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#to group by values within a column\n",
    "tracks_df.groupby('flight').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#grouping the whole set and perform function\n",
    "tracks_df.agg({'speed' : 'skewness'}).show() #ratio mean to sd is very low, wide spread observation negatively\n",
    "\n",
    "#can also use: avg(), count(), countDistinct(), first(), kurtosis(), max(), mean(), min(), skewness(), stddev(), stddev_pop(), stddev_samp(), sum(), sumDistinct(), var_pop(), var_samp() and variance().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#correlation is simple (only pearson , and in pairs)\n",
    "tracks_df.corr('landed','speed') #quit some relation which you expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def showCorrelationMatrix(df,numerical):\n",
    "    'for a DF print matrix with correlations between all numerical columns'\n",
    "    n_numerical = len(numerical)\n",
    "    corr = []\n",
    "    for i in range(0, n_numerical):\n",
    "        temp = [None] * i\n",
    "        for j in range(i, n_numerical):\n",
    "            temp.append(df.corr(numerical[i], numerical[j]))\n",
    "        corr.append(temp)\n",
    "        \n",
    "    print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "showCorrelationMatrix(tracks_df,['speed','landed','altitude','heading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tracks_df.corr('speed','altitude') #very high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lets try to make some features\n",
    "```\n",
    "Flight,time,speed,distance_to_ams,time_till_actual_landing\n",
    "\n",
    "distance to amsterdam , will be lat/lon comparison to lat/lon ams airport\n",
    "for each row\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "#This will be slow in pyspark due to context switching JVM and pyspark\n",
    "#Better to use UDF, or use Scala etc\n",
    "def dist_to_ams(lat,lon):\n",
    "    return haversine(float(4.761392), float(52.308871), float(lon),float(lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "udf_func = udf(dist_to_ams, FloatType())\n",
    "tracks_df = tracks_df.withColumn(\"distance_to_ams\", \\\n",
    "                            udf_func(tracks_df.lat,tracks_df.lon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Distance is great, however, after landing, the next row is the NEXT\n",
    "# flight\n",
    "df = tracks_df.where(\"flight == 'KL214'\").toPandas().sort_values(['time']).reset_index()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#just a test to compare rows\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = (\n",
    "    sc.parallelize([\n",
    "        (134, 30, \"2016-07-02 12:01:40\"), (134, 32, \"2016-07-02 12:21:23\"),\n",
    "        (125, 30, \"2016-07-02 13:22:56\"), (125, 32, \"2016-07-02 13:27:07\"),\n",
    "    ]).toDF([\"itemid\", \"eventid\", \"timestamp\"])\n",
    "    .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"itemid\").orderBy(\"timestamp\")\n",
    "\n",
    "diff = col(\"timestamp\").cast(\"long\") - lag(\"timestamp\", 1).over(w).cast(\"long\")\n",
    "\n",
    "df = df.withColumn(\"diff\", diff)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lets create a delta to get the landing times\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = (\n",
    "    sc.parallelize([\n",
    "        ('kl123', 0, \"2016-07-02 12:01:40\"), ('kl123', 0, \"2016-07-02 12:21:23\"),\n",
    "        ('kl123', 1, \"2016-07-02 13:22:56\"), ('kl123', 1, \"2016-07-02 13:27:07\"),\n",
    "    ]).toDF([\"itemid\", \"landed\", \"timestamp\"])\n",
    "    .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"itemid\").orderBy(\"timestamp\")\n",
    "\n",
    "diff = col(\"landed\").cast(\"int\") - lag(\"landed\", 1).over(w).cast(\"int\")\n",
    "\n",
    "df = df.withColumn(\"diff\", diff)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------+--------------+\n",
      "|flight|registration|runway|touchdown_time|\n",
      "+------+------------+------+--------------+\n",
      "|CND518|      PH-CDE|     2|    1466792627|\n",
      "| DL138|      N836MH|   171|    1466760692|\n",
      "|MP6742|      PH-MCP|   182|    1466778285|\n",
      "|KL1742|      PH-KZD|   182|    1466775981|\n",
      "|U28881|      G-EZDO|   132|    1466793964|\n",
      "|KL1134|      PH-BGX|   151|    1466796510|\n",
      "|KL1800|      PH-BGH|   182|    1466801922|\n",
      "|KL1858|      PH-KZP|   182|    1466777442|\n",
      "| KL888|      PH-BFP|   132|    1466785238|\n",
      "| OR288|      PH-TFD|   182|    1466767433|\n",
      "|KL1618|      PH-BXH|   165|    1466749916|\n",
      "|KL1790|      PH-BXC|   182|    1466749726|\n",
      "| KL736|      PH-BFA|   182|    1466743064|\n",
      "|U27908|      G-EZWP|   177|    1466800107|\n",
      "|KL1168|      PH-BCB|   160|    1466775337|\n",
      "|KL1010|      PH-BGO|   154|    1466769724|\n",
      "| LH992|      D-AIRY|   182|    1466770692|\n",
      "| LX736|      HB-IPV|   160|    1466799875|\n",
      "| BT621|      YL-BBE|   154|    1466751644|\n",
      "|U26771|      G-EZAP|   267|    1466763353|\n",
      "+------+------------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now lets adds column to show the landing moment, so delta of the landed column should be +1 .\n",
    "#\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "w = Window.partitionBy(\"flight\").orderBy(\"time\")\n",
    "\n",
    "diff = col(\"landed\").cast(\"int\") - lag(\"landed\", 1).over(w).cast(\"int\")\n",
    "\n",
    "tracks_touchdown_df = tracks_df.select([\"flight\",\"time\",\"heading\",\"landed\",\"registration\"]).withColumn(\"touchdown\", diff)\n",
    "tracks_touchdown_df = tracks_touchdown_df.where(\"touchdown == 1\").select(col(\"flight\"), \\\n",
    "                                                                         col(\"registration\"),\\\n",
    "                                                                         col(\"heading\").alias(\"runway\"), \\\n",
    "                                                                         col(\"time\").alias(\"touchdown_time\") )\n",
    "tracks_touchdown_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check for multiple landings 1 flight\n",
    "#tracks_touchdown_df.groupby([\"flight\",\"registration\"]).count().where(\"count > 1\").show()\n",
    "tracks_touchdown_df.groupby([\"flight\"]).count().where(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#remove them\n",
    "tracks_touchdown_df = tracks_touchdown_df.where(\"flight != 'HV6118' and flight != 'HV5134' and flight != 'KL1412' and flight != 'HV6332' and flight != 'HV5356' and flight != 'HV5314' and flight != 'TP668' and flight != 'HV6146' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check again\n",
    "tracks_touchdown_df.groupby([\"flight\"]).count().where(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#join tracks with our landing table and add column showing time till land\n",
    "join_df = tracks_df.join(tracks_touchdown_df,tracks_df.flight == tracks_touchdown_df.flight)\n",
    "join_df = join_df.withColumn(\"time_till_landing\",col(\"touchdown_time\") - col(\"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get rid of > 7000 sec , so we make sure flights on the next day are not using the landing of current day\n",
    "#join_df = join_df.where(\"time_till_landing > -7000\")\n",
    "join_df = join_df.where(\"time_till_landing > 0\") #ML REQUIRES POSITIVES!\n",
    "join_df = join_df.withColumn(\"time_till_landing_minutes\",join_df.time_till_landing / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tracks_df.select(\"flight\").distinct().count() #747 planes, might be all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = join_df.toPandas().query(\"flight == 'KL214'\").sort_values(['time']).reset_index()\n",
    "join_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_till_landing_minutes</th>\n",
       "      <th>distance_to_ams</th>\n",
       "      <th>speed</th>\n",
       "      <th>altitude</th>\n",
       "      <th>heading</th>\n",
       "      <th>runway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>394.533333</td>\n",
       "      <td>5565.607910</td>\n",
       "      <td>483</td>\n",
       "      <td>30000</td>\n",
       "      <td>295</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427.033333</td>\n",
       "      <td>6582.910645</td>\n",
       "      <td>526</td>\n",
       "      <td>33000</td>\n",
       "      <td>54</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367.966667</td>\n",
       "      <td>5485.398926</td>\n",
       "      <td>545</td>\n",
       "      <td>35000</td>\n",
       "      <td>64</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>544.066667</td>\n",
       "      <td>7958.743164</td>\n",
       "      <td>392</td>\n",
       "      <td>13500</td>\n",
       "      <td>35</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>328.966667</td>\n",
       "      <td>4331.586426</td>\n",
       "      <td>485</td>\n",
       "      <td>38000</td>\n",
       "      <td>357</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>406.050000</td>\n",
       "      <td>6265.900879</td>\n",
       "      <td>498</td>\n",
       "      <td>35000</td>\n",
       "      <td>28</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>255.233333</td>\n",
       "      <td>3804.589355</td>\n",
       "      <td>520</td>\n",
       "      <td>36000</td>\n",
       "      <td>68</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>238.816667</td>\n",
       "      <td>3084.533936</td>\n",
       "      <td>448</td>\n",
       "      <td>34000</td>\n",
       "      <td>273</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>217.883333</td>\n",
       "      <td>2715.295654</td>\n",
       "      <td>451</td>\n",
       "      <td>34000</td>\n",
       "      <td>286</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>214.666667</td>\n",
       "      <td>2670.526855</td>\n",
       "      <td>452</td>\n",
       "      <td>34000</td>\n",
       "      <td>273</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>115.350000</td>\n",
       "      <td>1420.034424</td>\n",
       "      <td>470</td>\n",
       "      <td>36000</td>\n",
       "      <td>309</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.183333</td>\n",
       "      <td>17.321127</td>\n",
       "      <td>150</td>\n",
       "      <td>1975</td>\n",
       "      <td>183</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>367.283333</td>\n",
       "      <td>5624.569336</td>\n",
       "      <td>521</td>\n",
       "      <td>37000</td>\n",
       "      <td>51</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47.533333</td>\n",
       "      <td>597.511108</td>\n",
       "      <td>543</td>\n",
       "      <td>38000</td>\n",
       "      <td>38</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.766667</td>\n",
       "      <td>33.500465</td>\n",
       "      <td>295</td>\n",
       "      <td>8825</td>\n",
       "      <td>5</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>299.566667</td>\n",
       "      <td>4282.784180</td>\n",
       "      <td>510</td>\n",
       "      <td>38000</td>\n",
       "      <td>334</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.950000</td>\n",
       "      <td>48.801395</td>\n",
       "      <td>360</td>\n",
       "      <td>12750</td>\n",
       "      <td>24</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>230.850000</td>\n",
       "      <td>3447.891113</td>\n",
       "      <td>580</td>\n",
       "      <td>37000</td>\n",
       "      <td>70</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>383.716667</td>\n",
       "      <td>5908.363281</td>\n",
       "      <td>530</td>\n",
       "      <td>35000</td>\n",
       "      <td>61</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>5343.837402</td>\n",
       "      <td>549</td>\n",
       "      <td>37000</td>\n",
       "      <td>50</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>588.416667</td>\n",
       "      <td>8568.955078</td>\n",
       "      <td>489</td>\n",
       "      <td>33000</td>\n",
       "      <td>17</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>361.366667</td>\n",
       "      <td>5562.990234</td>\n",
       "      <td>552</td>\n",
       "      <td>37000</td>\n",
       "      <td>45</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>250.350000</td>\n",
       "      <td>3325.495850</td>\n",
       "      <td>435</td>\n",
       "      <td>40000</td>\n",
       "      <td>294</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>380.750000</td>\n",
       "      <td>5628.379883</td>\n",
       "      <td>509</td>\n",
       "      <td>33000</td>\n",
       "      <td>73</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>305.483333</td>\n",
       "      <td>4736.375977</td>\n",
       "      <td>544</td>\n",
       "      <td>37000</td>\n",
       "      <td>61</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>211.616667</td>\n",
       "      <td>2628.154053</td>\n",
       "      <td>429</td>\n",
       "      <td>38000</td>\n",
       "      <td>289</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>138.600000</td>\n",
       "      <td>1614.003662</td>\n",
       "      <td>427</td>\n",
       "      <td>38000</td>\n",
       "      <td>248</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>317.300000</td>\n",
       "      <td>4690.515625</td>\n",
       "      <td>500</td>\n",
       "      <td>37000</td>\n",
       "      <td>59</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>308.816667</td>\n",
       "      <td>4628.398926</td>\n",
       "      <td>529</td>\n",
       "      <td>35000</td>\n",
       "      <td>59</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>348.916667</td>\n",
       "      <td>5354.598145</td>\n",
       "      <td>541</td>\n",
       "      <td>37000</td>\n",
       "      <td>46</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84349</th>\n",
       "      <td>126.216667</td>\n",
       "      <td>1697.020508</td>\n",
       "      <td>476</td>\n",
       "      <td>36000</td>\n",
       "      <td>23</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84350</th>\n",
       "      <td>11.816667</td>\n",
       "      <td>29.791769</td>\n",
       "      <td>293</td>\n",
       "      <td>7050</td>\n",
       "      <td>42</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84351</th>\n",
       "      <td>121.733333</td>\n",
       "      <td>1479.681274</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84352</th>\n",
       "      <td>113.366667</td>\n",
       "      <td>1512.722290</td>\n",
       "      <td>470</td>\n",
       "      <td>36000</td>\n",
       "      <td>29</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84353</th>\n",
       "      <td>153.550000</td>\n",
       "      <td>2095.003174</td>\n",
       "      <td>472</td>\n",
       "      <td>36000</td>\n",
       "      <td>37</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84354</th>\n",
       "      <td>15.650000</td>\n",
       "      <td>51.967583</td>\n",
       "      <td>313</td>\n",
       "      <td>9000</td>\n",
       "      <td>41</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84355</th>\n",
       "      <td>39.200000</td>\n",
       "      <td>419.252808</td>\n",
       "      <td>514</td>\n",
       "      <td>38000</td>\n",
       "      <td>34</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84356</th>\n",
       "      <td>131.116667</td>\n",
       "      <td>1638.396362</td>\n",
       "      <td>473</td>\n",
       "      <td>36000</td>\n",
       "      <td>332</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84357</th>\n",
       "      <td>48.750000</td>\n",
       "      <td>556.710693</td>\n",
       "      <td>508</td>\n",
       "      <td>40000</td>\n",
       "      <td>17</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84358</th>\n",
       "      <td>37.300000</td>\n",
       "      <td>371.664276</td>\n",
       "      <td>455</td>\n",
       "      <td>37000</td>\n",
       "      <td>125</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84359</th>\n",
       "      <td>3.150000</td>\n",
       "      <td>16.842112</td>\n",
       "      <td>162</td>\n",
       "      <td>1950</td>\n",
       "      <td>184</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84360</th>\n",
       "      <td>137.600000</td>\n",
       "      <td>1738.581665</td>\n",
       "      <td>454</td>\n",
       "      <td>26750</td>\n",
       "      <td>345</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84361</th>\n",
       "      <td>65.783333</td>\n",
       "      <td>810.851074</td>\n",
       "      <td>484</td>\n",
       "      <td>37000</td>\n",
       "      <td>29</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84362</th>\n",
       "      <td>24.416667</td>\n",
       "      <td>142.881241</td>\n",
       "      <td>380</td>\n",
       "      <td>21700</td>\n",
       "      <td>308</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84363</th>\n",
       "      <td>88.550000</td>\n",
       "      <td>1143.265991</td>\n",
       "      <td>343</td>\n",
       "      <td>11100</td>\n",
       "      <td>318</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84364</th>\n",
       "      <td>55.933333</td>\n",
       "      <td>691.142578</td>\n",
       "      <td>503</td>\n",
       "      <td>36000</td>\n",
       "      <td>37</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84365</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>30.055099</td>\n",
       "      <td>289</td>\n",
       "      <td>6875</td>\n",
       "      <td>24</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84366</th>\n",
       "      <td>29.766667</td>\n",
       "      <td>249.682556</td>\n",
       "      <td>524</td>\n",
       "      <td>35075</td>\n",
       "      <td>35</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84367</th>\n",
       "      <td>20.233333</td>\n",
       "      <td>110.028458</td>\n",
       "      <td>405</td>\n",
       "      <td>17875</td>\n",
       "      <td>25</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84368</th>\n",
       "      <td>72.200000</td>\n",
       "      <td>932.590088</td>\n",
       "      <td>470</td>\n",
       "      <td>38000</td>\n",
       "      <td>36</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84369</th>\n",
       "      <td>17.116667</td>\n",
       "      <td>74.754036</td>\n",
       "      <td>356</td>\n",
       "      <td>14650</td>\n",
       "      <td>25</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84370</th>\n",
       "      <td>112.166667</td>\n",
       "      <td>1500.025757</td>\n",
       "      <td>468</td>\n",
       "      <td>38000</td>\n",
       "      <td>27</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84371</th>\n",
       "      <td>45.333333</td>\n",
       "      <td>426.052612</td>\n",
       "      <td>465</td>\n",
       "      <td>38000</td>\n",
       "      <td>302</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84372</th>\n",
       "      <td>80.450000</td>\n",
       "      <td>955.040161</td>\n",
       "      <td>466</td>\n",
       "      <td>38000</td>\n",
       "      <td>321</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84373</th>\n",
       "      <td>62.316667</td>\n",
       "      <td>737.129395</td>\n",
       "      <td>466</td>\n",
       "      <td>38000</td>\n",
       "      <td>349</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84374</th>\n",
       "      <td>48.616667</td>\n",
       "      <td>534.979797</td>\n",
       "      <td>488</td>\n",
       "      <td>38000</td>\n",
       "      <td>343</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84375</th>\n",
       "      <td>45.400000</td>\n",
       "      <td>486.719604</td>\n",
       "      <td>495</td>\n",
       "      <td>38000</td>\n",
       "      <td>343</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84376</th>\n",
       "      <td>42.233333</td>\n",
       "      <td>529.370300</td>\n",
       "      <td>528</td>\n",
       "      <td>38000</td>\n",
       "      <td>20</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84377</th>\n",
       "      <td>26.266667</td>\n",
       "      <td>263.327057</td>\n",
       "      <td>544</td>\n",
       "      <td>38000</td>\n",
       "      <td>21</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84378</th>\n",
       "      <td>11.733333</td>\n",
       "      <td>41.723511</td>\n",
       "      <td>297</td>\n",
       "      <td>10850</td>\n",
       "      <td>291</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84379 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time_till_landing_minutes  distance_to_ams  speed  altitude  heading  \\\n",
       "0                     394.533333      5565.607910    483     30000      295   \n",
       "1                     427.033333      6582.910645    526     33000       54   \n",
       "2                     367.966667      5485.398926    545     35000       64   \n",
       "3                     544.066667      7958.743164    392     13500       35   \n",
       "4                     328.966667      4331.586426    485     38000      357   \n",
       "5                     406.050000      6265.900879    498     35000       28   \n",
       "6                     255.233333      3804.589355    520     36000       68   \n",
       "7                     238.816667      3084.533936    448     34000      273   \n",
       "8                     217.883333      2715.295654    451     34000      286   \n",
       "9                     214.666667      2670.526855    452     34000      273   \n",
       "10                    115.350000      1420.034424    470     36000      309   \n",
       "11                      3.183333        17.321127    150      1975      183   \n",
       "12                    367.283333      5624.569336    521     37000       51   \n",
       "13                     47.533333       597.511108    543     38000       38   \n",
       "14                     11.766667        33.500465    295      8825        5   \n",
       "15                    299.566667      4282.784180    510     38000      334   \n",
       "16                     13.950000        48.801395    360     12750       24   \n",
       "17                    230.850000      3447.891113    580     37000       70   \n",
       "18                    383.716667      5908.363281    530     35000       61   \n",
       "19                    350.000000      5343.837402    549     37000       50   \n",
       "20                    588.416667      8568.955078    489     33000       17   \n",
       "21                    361.366667      5562.990234    552     37000       45   \n",
       "22                    250.350000      3325.495850    435     40000      294   \n",
       "23                    380.750000      5628.379883    509     33000       73   \n",
       "24                    305.483333      4736.375977    544     37000       61   \n",
       "25                    211.616667      2628.154053    429     38000      289   \n",
       "26                    138.600000      1614.003662    427     38000      248   \n",
       "27                    317.300000      4690.515625    500     37000       59   \n",
       "28                    308.816667      4628.398926    529     35000       59   \n",
       "29                    348.916667      5354.598145    541     37000       46   \n",
       "...                          ...              ...    ...       ...      ...   \n",
       "84349                 126.216667      1697.020508    476     36000       23   \n",
       "84350                  11.816667        29.791769    293      7050       42   \n",
       "84351                 121.733333      1479.681274     34         0      205   \n",
       "84352                 113.366667      1512.722290    470     36000       29   \n",
       "84353                 153.550000      2095.003174    472     36000       37   \n",
       "84354                  15.650000        51.967583    313      9000       41   \n",
       "84355                  39.200000       419.252808    514     38000       34   \n",
       "84356                 131.116667      1638.396362    473     36000      332   \n",
       "84357                  48.750000       556.710693    508     40000       17   \n",
       "84358                  37.300000       371.664276    455     37000      125   \n",
       "84359                   3.150000        16.842112    162      1950      184   \n",
       "84360                 137.600000      1738.581665    454     26750      345   \n",
       "84361                  65.783333       810.851074    484     37000       29   \n",
       "84362                  24.416667       142.881241    380     21700      308   \n",
       "84363                  88.550000      1143.265991    343     11100      318   \n",
       "84364                  55.933333       691.142578    503     36000       37   \n",
       "84365                  10.500000        30.055099    289      6875       24   \n",
       "84366                  29.766667       249.682556    524     35075       35   \n",
       "84367                  20.233333       110.028458    405     17875       25   \n",
       "84368                  72.200000       932.590088    470     38000       36   \n",
       "84369                  17.116667        74.754036    356     14650       25   \n",
       "84370                 112.166667      1500.025757    468     38000       27   \n",
       "84371                  45.333333       426.052612    465     38000      302   \n",
       "84372                  80.450000       955.040161    466     38000      321   \n",
       "84373                  62.316667       737.129395    466     38000      349   \n",
       "84374                  48.616667       534.979797    488     38000      343   \n",
       "84375                  45.400000       486.719604    495     38000      343   \n",
       "84376                  42.233333       529.370300    528     38000       20   \n",
       "84377                  26.266667       263.327057    544     38000       21   \n",
       "84378                  11.733333        41.723511    297     10850      291   \n",
       "\n",
       "       runway  \n",
       "0         143  \n",
       "1         182  \n",
       "2         129  \n",
       "3         244  \n",
       "4         151  \n",
       "5         180  \n",
       "6         182  \n",
       "7         182  \n",
       "8         180  \n",
       "9         180  \n",
       "10        168  \n",
       "11        182  \n",
       "12        182  \n",
       "13        182  \n",
       "14        132  \n",
       "15        177  \n",
       "16        154  \n",
       "17        182  \n",
       "18        182  \n",
       "19        182  \n",
       "20        264  \n",
       "21        180  \n",
       "22        182  \n",
       "23        182  \n",
       "24        165  \n",
       "25        137  \n",
       "26        174  \n",
       "27        151  \n",
       "28        180  \n",
       "29        180  \n",
       "...       ...  \n",
       "84349     182  \n",
       "84350     182  \n",
       "84351     182  \n",
       "84352     182  \n",
       "84353     180  \n",
       "84354     160  \n",
       "84355     182  \n",
       "84356     146  \n",
       "84357     182  \n",
       "84358     168  \n",
       "84359     182  \n",
       "84360     182  \n",
       "84361     182  \n",
       "84362     182  \n",
       "84363     182  \n",
       "84364     182  \n",
       "84365     182  \n",
       "84366     182  \n",
       "84367     182  \n",
       "84368     180  \n",
       "84369     182  \n",
       "84370     129  \n",
       "84371     146  \n",
       "84372     149  \n",
       "84373     182  \n",
       "84374     182  \n",
       "84375     182  \n",
       "84376     182  \n",
       "84377     182  \n",
       "84378     149  \n",
       "\n",
       "[84379 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Create our machine learing dataset\n",
    "ml_df = join_df.select(['time_till_landing_minutes','distance_to_ams','speed','altitude','heading','runway'])\n",
    "#ml_df = ml_df.withColumn('id',fn.monotonically_increasing_id())\n",
    "ml_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, testing_data = ml_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ml_df.toPandas().to_csv('/root/fr24/fr24.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# we need to convert categorical variables to numeric ones first\n",
    "#isLondonIndexer = StringIndexer(inputCol=\"isLondon\", outputCol=\"isLondonI\")\n",
    "#durationIndexer = StringIndexer(inputCol=\"duration\", outputCol=\"durationI\")\n",
    "#typeIndexer = StringIndexer(inputCol=\"type\", outputCol=\"typeI\")\n",
    "# then we convert these into a feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"distance_to_ams\",\"speed\",\"altitude\",\"heading\",\"runway\"],outputCol=\"features\")\n",
    "# and we pick a simple model to test out\n",
    "Regressor = DecisionTreeRegressor(featuresCol=\"features\",labelCol=\"time_till_landing_minutes\",maxDepth=25)\n",
    "\n",
    "#pipeline = Pipeline(stages=[isLondonIndexer,durationIndexer,typeIndexer,assembler,Regressor])\n",
    "pipeline = Pipeline(stages=[assembler,Regressor])\n",
    "model = pipeline.fit(training_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testing_data)\n",
    "modelEvaluator = RegressionEvaluator(labelCol=\"time_till_landing_minutes\")\n",
    "modelError = modelEvaluator.evaluate(predictions) #rmse by default\n",
    "modelError = modelEvaluator.evaluate(predictions,{modelEvaluator.metricName: \"mae\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelError #5mins deviation, at some point it was 4,5 minutes. not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9b7b2544b620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#pipeline = Pipeline(stages=[isLondonIndexer,durationIndexer,typeIndexer,assembler,Regressor])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRegressor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"distance_to_ams\",\"speed\",\"altitude\",\"heading\",\"runway\"],outputCol=\"features\")\n",
    "Regressor = RandomForestRegressor(featuresCol=\"features\",labelCol=\"time_till_landing_minutes\",maxDepth=5)\n",
    "\n",
    "#pipeline = Pipeline(stages=[isLondonIndexer,durationIndexer,typeIndexer,assembler,Regressor])\n",
    "pipeline = Pipeline(stages=[assembler,Regressor])\n",
    "model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test again with new model\n",
    "predictions = model.transform(testing_data)\n",
    "modelEvaluator = RegressionEvaluator(labelCol=\"time_till_landing_minutes\")\n",
    "modelError = modelEvaluator.evaluate(predictions) #rmse by default\n",
    "modelError = modelEvaluator.evaluate(predictions,{modelEvaluator.metricName: \"mae\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.435056439421997"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#22 mins with default settings depth 5, 9 mins with depth 10.\n",
    "#6,4 with depth 15. Crashed with 20 depth on mac\n",
    "modelError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification example\n",
    "\n",
    "```\n",
    "import pyspark.ml.classification as cl\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "encoder = ft.OneHotEncoder(\n",
    "       inputCol='BIRTH_PLACE_INT',\n",
    "       outputCol='BIRTH_PLACE_VEC')\n",
    "       \n",
    "\n",
    "logistic = cl.LogisticRegression(\n",
    "       maxIter=10,\n",
    "       regParam=0.01,\n",
    "       labelCol='some_classifation')\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "           encoder,\n",
    "           featuresCreator,logistic ])\n",
    "    \n",
    "births_train, births_test = births \\\n",
    "       .randomSplit([0.7, 0.3], seed=666)\n",
    "    \n",
    "model = pipeline.fit(births_train)\n",
    "test_model = model.transform(births_test)  \n",
    "\n",
    "valuator = ev.BinaryClassificationEvaluator(\n",
    "       rawPredictionCol='probability',\n",
    "       labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "       \n",
    "pipelinePath = './infant_oneHotEncoder_Logistic_Pipeline'\n",
    "pipeline.write().overwrite().save(pipelinePath)\n",
    "\n",
    "to load:\n",
    "loadedPipeline = Pipeline.load(pipelinePath)\n",
    "loadedPipeline \\\n",
    "       .fit(births_train)\\\n",
    "       .transform(births_test)\\\n",
    "       .take(1)\n",
    "       \n",
    "       \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.toPandas().head(10) #show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "What would happen if we have a string based column?\n",
    "\n",
    "Make it a number\n",
    "births = births \\\n",
    "       .withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE'] \\\n",
    "       .cast(typ.IntegerType()))\n",
    "       \n",
    "       \n",
    "   encoder = ft.OneHotEncoder(\n",
    "       inputCol='BIRTH_PLACE_INT',\n",
    "       outputCol='BIRTH_PLACE_VEC')\n",
    "       \n",
    "Like before create the assembler, but we use getoutputcol so we \n",
    "dont care about the real column name:\n",
    "   featuresCreator = ft.VectorAssembler(\n",
    "       inputCols=[\n",
    "           col[0]\n",
    "           for col\n",
    "           in labels[2:]] + \\\n",
    "       [encoder.getOutputCol()],\n",
    "     outputCol='features'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import bokeh.charts as chrt\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ml_df.where(\"time_till_landing_minutes < 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hists = ml_df.select('time_till_landing_minutes').rdd.flatMap(\n",
    "       lambda row: row\n",
    ").histogram(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_hist = {\n",
    "       'bins': hists[0][:-1],\n",
    "       'freq': hists[1]\n",
    "   }\n",
    "plt.bar(data_hist['bins'], data_hist['freq'], width=20)\n",
    "plt.title('Histogram of \\'time_till_landing\\'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "b_hist = chrt.Bar(\n",
    "       data_hist,\n",
    "       values='freq', label='bins',\n",
    "       title='Histogram of \\'balance\\'')\n",
    "chrt.show(b_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_back = 0.001\n",
    "\n",
    "# use this if you want an (almost) exact number of samples\n",
    "# sample_count = 200\n",
    "# percent_back = sample_count / posts.count()\n",
    "\n",
    "frac = dict(\n",
    "    (e.time_till_landing_minutes, percent_back) \n",
    "    for e \n",
    "    in ml_df.select('time_till_landing_minutes').distinct().collect()\n",
    ")\n",
    "sampled = ml_df.sampleBy('time_till_landing_minutes', fractions=frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sampled.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
